package ddpg;

// Privacy options. Feel free to ignore (unless you want to reproduce the results shown in "TODO").
// Next ID: 26.
message PrivacyOptions {
  enum Mode {
    ALTERNATE = 0;
    SIMULTANEOUS = 1;
  }
  optional Mode mode = 1 [default = ALTERNATE];

  // In alternate mode, ddpg is trained (with exploration) for ddpg_training_episodes followed by
  // privacy_training_episode where the agent is deterministic.
  optional int32 ddpg_training_episodes = 2 [default = 10];
  optional int32 privacy_training_episodes = 3 [default = 40];

  optional bool save_trajectories = 4 [default = true];

  // Training options for the ALTERNATE and SIMULTANEOUS modes.
  optional int32 switch_timestep = 5 [default = 40];
  optional double switch_privacy_slope = 6 [default = 0.2];
  optional double switch_privacy_min = 7 [default = 0];
  optional double switch_privacy_max = 8 [default = 1];
  optional double privacy_multiplier = 9 [default = 1];
  optional double privacy_smoothing_decay = 25 [default = 0.2];
  optional double switch_performance_slope = 10 [default = 0.1];
  optional double switch_performance_min = 11 [default = 0.1];
  optional double switch_performance_max = 12 [default = 1];
  optional double performance_multiplier = 13 [default = 1];
  optional int32 batch_size = 14 [default = 64];
  optional int32 train_every_n_timesteps = 15 [default = 1];
  optional int32 add_to_replay_memory_until_timestep = 16 [default = 60];
  optional int32 warmup_timesteps = 17 [default = 800];
  optional int32 replay_memory_size = 18 [default = 800];
  optional bool use_uniform_weights = 19 [default = true];
  repeated int32 layer_size = 20;  // If not set, assumes [10].
  optional double weight_decay = 21 [default = 1e-2];
  optional double tau = 22 [default = 1e-1];
  optional double learning_rate = 23 [default = 1e-2];

  // Trains the model with or without providing rewards.
  optional bool apply_reward = 24 [default = true];
}
